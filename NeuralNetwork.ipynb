{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 2: Activation Functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 3: Data Preprocessing\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoded vectors\n",
    "    y: labels (can be 1D array, column vector, or row vector)\n",
    "    num_classes: number of classes\n",
    "    \"\"\"\n",
    "    y = np.array(y).reshape(-1)\n",
    "    m = len(y)\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    \n",
    "    if num_classes == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    \n",
    "    one_hot[np.arange(m), y.astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Neural Network Core Functions\n",
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layers)):\n",
    "        parameters[f'W{i}'] = np.random.randn(layers[i], layers[i - 1]) * 0.01\n",
    "        parameters[f'b{i}'] = np.zeros((layers[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forward_propagation(X, parameters, layers):\n",
    "    cache = {\"A0\": X}\n",
    "    A = X\n",
    "    for i in range(1, len(layers)-1):\n",
    "        Z = np.dot(parameters[f'W{i}'], A.T) + parameters[f'b{i}']\n",
    "        A = relu(Z.T)\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "    Z = np.dot(parameters[f'W{len(layers)-1}'], A.T) + parameters[f'b{len(layers)-1}']\n",
    "    A = sigmoid(Z.T)\n",
    "    cache[f\"Z{len(layers)-1}\"], cache[f\"A{len(layers)-1}\"] = Z, A\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def back_propagation(y, parameters, cache, layers):\n",
    "    grads = {}\n",
    "    m = y.shape[0]\n",
    "    L = len(layers) - 1\n",
    "    \n",
    "    if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "        y = to_one_hot(y, layers[-1])\n",
    "    \n",
    "    A = cache[f\"A{L}\"]\n",
    "    dZ = A - y\n",
    "    \n",
    "    for i in reversed(range(1, L+1)):\n",
    "        dW = (1/m) * np.dot(dZ.T, cache[f\"A{i-1}\"])\n",
    "        db = (1/m) * np.sum(dZ, axis=0).reshape(-1, 1)\n",
    "        \n",
    "        grads[f\"dW{i}\"], grads[f\"db{i}\"] = dW, db\n",
    "        \n",
    "        if i > 1:\n",
    "            dA = np.dot(dZ, parameters[f\"W{i}\"])\n",
    "            dZ = dA * relu_derivative(cache[f\"Z{i-1}\"].T)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update network parameters using gradients\n",
    "    \"\"\"\n",
    "    for i in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f\"W{i}\"] -= learning_rate * grads[f\"dW{i}\"]\n",
    "        parameters[f\"b{i}\"] -= learning_rate * grads[f\"db{i}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Cell 5: Training Function\n",
    "def train(X, y, layers, learning_rate=0.1, epochs=1000):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    for epoch in range(epochs):\n",
    "        y_pred, cache = forward_propagation(X, parameters, layers)\n",
    "        loss = compute_loss(y, y_pred)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        grads = back_propagation(y, parameters, cache, layers)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "\n",
    "# Cell 6: Prediction Function\n",
    "def predict(X, parameters, layers):\n",
    "    y_pred, _ = forward_propagation(X, parameters, layers)\n",
    "    return y_pred > 0.5\n",
    "\n",
    "# Cell 7: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6931471537880713\n",
      "Epoch 100, Loss: 0.5641017845788526\n",
      "Epoch 200, Loss: 0.5623718511098156\n",
      "Epoch 300, Loss: 0.5623359302024674\n",
      "Epoch 400, Loss: 0.5623351338422893\n",
      "Epoch 500, Loss: 0.5623351158129316\n",
      "Epoch 600, Loss: 0.562335115473\n",
      "Epoch 700, Loss: 0.562335115207304\n",
      "Epoch 800, Loss: 0.5623351149941209\n",
      "Epoch 900, Loss: 0.5623351150504285\n",
      "Predictions: [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define example data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Shape: (4, 2)\n",
    "y = np.array([[0], [1], [1], [1]])  # Shape: (4, 1)\n",
    "\n",
    "# Define and train the network\n",
    "layers = [2, 3, 4, 4, 1]\n",
    "parameters = train(X, y, layers)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X, parameters, layers)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
