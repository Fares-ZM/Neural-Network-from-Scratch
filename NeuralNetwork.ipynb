{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 2: Activation Functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(Z):\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function\n",
    "    Z: Input numpy array\n",
    "    Returns: tanh(Z)\n",
    "    \"\"\"\n",
    "    return np.tanh(Z)\n",
    "\n",
    "def tanh_derivative(Z):\n",
    "    \"\"\"\n",
    "    Derivative of tanh activation function\n",
    "    Z: Input numpy array\n",
    "    Returns: 1 - tanhÂ²(Z)\n",
    "    \"\"\"\n",
    "    return 1 - np.square(np.tanh(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 3: Data Preprocessing\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoded vectors\n",
    "    y: labels (can be 1D array, column vector, or row vector)\n",
    "    num_classes: number of classes\n",
    "    \"\"\"\n",
    "    y = np.array(y).reshape(-1)\n",
    "    m = len(y)\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    \n",
    "    if num_classes == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    \n",
    "    one_hot[np.arange(m), y.astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Neural Network Core Functions\n",
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layers)):\n",
    "        parameters[f'W{i}'] = np.random.randn(layers[i], layers[i - 1]) * 0.01\n",
    "        parameters[f'b{i}'] = np.zeros((layers[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layers, hidden_activation, output_activation):\n",
    "    cache = {\"A0\": X}\n",
    "    A = X\n",
    "    for i in range(1, len(layers)-1):\n",
    "        Z = np.dot(parameters[f'W{i}'], A.T) + parameters[f'b{i}']\n",
    "        A = hidden_activation(Z.T)\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "    Z = np.dot(parameters[f'W{len(layers)-1}'], A.T) + parameters[f'b{len(layers)-1}']\n",
    "    A = output_activation(Z.T)\n",
    "    cache[f\"Z{len(layers)-1}\"], cache[f\"A{len(layers)-1}\"] = Z, A\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def compute_loss(y_true, y_pred):\n",
    "#     m = y_true.shape[0]\n",
    "#     loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def back_propagation(y, parameters, cache, layers):\n",
    "#     grads = {}\n",
    "#     m = y.shape[0]\n",
    "#     L = len(layers) - 1\n",
    "    \n",
    "#     if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "#         y = to_one_hot(y, layers[-1])\n",
    "    \n",
    "#     A = cache[f\"A{L}\"]\n",
    "#     dZ = A - y\n",
    "    \n",
    "#     for i in reversed(range(1, L+1)):\n",
    "#         dW = (1/m) * np.dot(dZ.T, cache[f\"A{i-1}\"])\n",
    "#         db = (1/m) * np.sum(dZ, axis=0).reshape(-1, 1)\n",
    "        \n",
    "#         grads[f\"dW{i}\"], grads[f\"db{i}\"] = dW, db\n",
    "        \n",
    "#         if i > 1:\n",
    "#             dA = np.dot(dZ, parameters[f\"W{i}\"])\n",
    "#             dZ = dA * relu_derivative(cache[f\"Z{i-1}\"].T)\n",
    "    \n",
    "#     return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, parameters, cache, layers, hidden_activation_derivative):\n",
    "    grads = {}\n",
    "    m = y.shape[0]\n",
    "    L = len(layers) - 1\n",
    "    \n",
    "    # For the output layer (using softmax)\n",
    "    dZ = cache[f\"A{L}\"] - y\n",
    "    \n",
    "    for i in reversed(range(1, L+1)):\n",
    "        dW = (1/m) * np.dot(dZ.T, cache[f\"A{i-1}\"])\n",
    "        db = (1/m) * np.sum(dZ, axis=0, keepdims=True).T\n",
    "        \n",
    "        grads[f\"dW{i}\"], grads[f\"db{i}\"] = dW, db\n",
    "        \n",
    "        if i > 1:\n",
    "            dA = np.dot(dZ, parameters[f\"W{i}\"])\n",
    "            dZ = dA * hidden_activation_derivative(cache[f\"Z{i-1}\"].T)\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update network parameters using gradients\n",
    "    \"\"\"\n",
    "    for i in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f\"W{i}\"] -= learning_rate * grads[f\"dW{i}\"]\n",
    "        parameters[f\"b{i}\"] -= learning_rate * grads[f\"db{i}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #Cell 5: Training Function\n",
    "# def train(X, y, layers, learning_rate=0.1, epochs=1000):\n",
    "#     parameters = initialize_parameters(layers)\n",
    "#     for epoch in range(epochs):\n",
    "#         y_pred, cache = forward_propagation(X, parameters, layers)\n",
    "#         loss = compute_loss(y, y_pred)\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "#         grads = back_propagation(y, parameters, cache, layers)\n",
    "#         parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, y, layers, learning_rate=0.01, epochs=1000, batch_size=32, \n",
    "                 hidden_activation=relu, output_activation=softmax,\n",
    "                 hidden_activation_derivative=relu_derivative):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Ensure batch_size is not larger than dataset\n",
    "    batch_size = min(batch_size, n_samples)\n",
    "    n_batches = max(n_samples // batch_size, 1)  # Ensure at least 1 batch\n",
    "    \n",
    "    print(\"Training Progress:\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for batch in range(n_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass with specified activation functions\n",
    "            y_pred, cache = forward_propagation(X_batch, parameters, layers, \n",
    "                                              hidden_activation, output_activation)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(y_batch, y_pred)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass with specified derivative\n",
    "            grads = back_propagation(y_batch, parameters, cache, layers, \n",
    "                                   hidden_activation_derivative)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, layers, hidden_activation, output_activation):\n",
    "    y_pred, _ = forward_propagation(X, parameters, layers, hidden_activation, output_activation)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from CSV files...\n",
      "Training data shape: (59999, 784)\n",
      "Training labels shape: (59999, 10)\n",
      "Number of classes: 10\n",
      "\n",
      "Training the network...\n",
      "Training Progress:\n",
      "Epoch 0/100, Loss: 1.1633\n",
      "Epoch 10/100, Loss: 0.0947\n",
      "Epoch 20/100, Loss: 0.0391\n",
      "Epoch 30/100, Loss: 0.0166\n",
      "Epoch 40/100, Loss: 0.0078\n",
      "Epoch 50/100, Loss: 0.0043\n",
      "Epoch 60/100, Loss: 0.0027\n",
      "Epoch 70/100, Loss: 0.0019\n",
      "Epoch 80/100, Loss: 0.0015\n",
      "Epoch 90/100, Loss: 0.0012\n",
      "\n",
      "Test Accuracy: 97.87%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Now use the new function name in your training code:\n",
    "print(\"Loading dataset from CSV files...\")\n",
    "train_data = pd.read_csv(\"data/emnist-mnist-train.csv\")\n",
    "test_data = pd.read_csv(\"data/emnist-mnist-test.csv\")\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, 1:].values  # All columns except first\n",
    "y_train = train_data.iloc[:, 0].values   # First column contains labels\n",
    "\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_one_hot = to_one_hot(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_one_hot(y_test, num_classes=num_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_one_hot.shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Define network architecture\n",
    "input_size = X_train.shape[1]  # 784 features\n",
    "layers = [input_size, 256, 128, num_classes]\n",
    "\n",
    "# Train the network using the new function name\n",
    "print(\"\\nTraining the network...\")\n",
    "parameters = train_network(X_train, y_train_one_hot, \n",
    "                         layers=layers,\n",
    "                         learning_rate=0.01,\n",
    "                         epochs=100,\n",
    "                         batch_size=32, \n",
    "                        hidden_activation=tanh,\n",
    "                        hidden_activation_derivative=tanh_derivative,\n",
    "                        output_activation=softmax)\n",
    "\n",
    "# Test the model\n",
    "predictions = predict(X_test, parameters, layers, tanh, softmax)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "actual_classes = y_test\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == actual_classes)\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
