{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 2: Activation Functions\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return Z > 0\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def softmax(Z):\n",
    "    return np.exp(Z) / np.sum(np.exp(Z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Cell 3: Data Preprocessing\n",
    "def to_one_hot(y, num_classes):\n",
    "    \"\"\"\n",
    "    Convert class labels to one-hot encoded vectors\n",
    "    y: labels (can be 1D array, column vector, or row vector)\n",
    "    num_classes: number of classes\n",
    "    \"\"\"\n",
    "    y = np.array(y).reshape(-1)\n",
    "    m = len(y)\n",
    "    one_hot = np.zeros((m, num_classes))\n",
    "    \n",
    "    if num_classes == 1:\n",
    "        return y.reshape(-1, 1)\n",
    "    \n",
    "    one_hot[np.arange(m), y.astype(int)] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: Neural Network Core Functions\n",
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layers)):\n",
    "        parameters[f'W{i}'] = np.random.randn(layers[i], layers[i - 1]) * 0.01\n",
    "        parameters[f'b{i}'] = np.zeros((layers[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def forward_propagation(X, parameters, layers):\n",
    "    cache = {\"A0\": X}\n",
    "    A = X\n",
    "    for i in range(1, len(layers)-1):\n",
    "        Z = np.dot(parameters[f'W{i}'], A.T) + parameters[f'b{i}']\n",
    "        A = relu(Z.T)\n",
    "        cache[f\"Z{i}\"], cache[f\"A{i}\"] = Z, A\n",
    "    Z = np.dot(parameters[f'W{len(layers)-1}'], A.T) + parameters[f'b{len(layers)-1}']\n",
    "    A = softmax(Z.T)\n",
    "    cache[f\"Z{len(layers)-1}\"], cache[f\"A{len(layers)-1}\"] = Z, A\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def compute_loss(y_true, y_pred):\n",
    "#     m = y_true.shape[0]\n",
    "#     loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Categorical cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.sum(y_true * np.log(y_pred)) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def back_propagation(y, parameters, cache, layers):\n",
    "#     grads = {}\n",
    "#     m = y.shape[0]\n",
    "#     L = len(layers) - 1\n",
    "    \n",
    "#     if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "#         y = to_one_hot(y, layers[-1])\n",
    "    \n",
    "#     A = cache[f\"A{L}\"]\n",
    "#     dZ = A - y\n",
    "    \n",
    "#     for i in reversed(range(1, L+1)):\n",
    "#         dW = (1/m) * np.dot(dZ.T, cache[f\"A{i-1}\"])\n",
    "#         db = (1/m) * np.sum(dZ, axis=0).reshape(-1, 1)\n",
    "        \n",
    "#         grads[f\"dW{i}\"], grads[f\"db{i}\"] = dW, db\n",
    "        \n",
    "#         if i > 1:\n",
    "#             dA = np.dot(dZ, parameters[f\"W{i}\"])\n",
    "#             dZ = dA * relu_derivative(cache[f\"Z{i-1}\"].T)\n",
    "    \n",
    "#     return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(y, parameters, cache, layers):\n",
    "    grads = {}\n",
    "    m = y.shape[0]\n",
    "    L = len(layers) - 1\n",
    "    \n",
    "    # For the output layer (using softmax)\n",
    "    dZ = cache[f\"A{L}\"] - y\n",
    "    \n",
    "    for i in reversed(range(1, L+1)):\n",
    "        dW = (1/m) * np.dot(dZ.T, cache[f\"A{i-1}\"])\n",
    "        db = (1/m) * np.sum(dZ, axis=0, keepdims=True).T\n",
    "        \n",
    "        grads[f\"dW{i}\"], grads[f\"db{i}\"] = dW, db\n",
    "        \n",
    "        if i > 1:\n",
    "            dA = np.dot(dZ, parameters[f\"W{i}\"])\n",
    "            dZ = dA * relu_derivative(cache[f\"Z{i-1}\"].T)\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update network parameters using gradients\n",
    "    \"\"\"\n",
    "    for i in range(1, len(parameters) // 2 + 1):\n",
    "        parameters[f\"W{i}\"] -= learning_rate * grads[f\"dW{i}\"]\n",
    "        parameters[f\"b{i}\"] -= learning_rate * grads[f\"db{i}\"]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  #Cell 5: Training Function\n",
    "# def train(X, y, layers, learning_rate=0.1, epochs=1000):\n",
    "#     parameters = initialize_parameters(layers)\n",
    "#     for epoch in range(epochs):\n",
    "#         y_pred, cache = forward_propagation(X, parameters, layers)\n",
    "#         loss = compute_loss(y, y_pred)\n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "#         grads = back_propagation(y, parameters, cache, layers)\n",
    "#         parameters = update_parameters(parameters, grads, learning_rate)\n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, y, layers, learning_rate=0.01, epochs=1000, batch_size=32):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Ensure batch_size is not larger than dataset\n",
    "    batch_size = min(batch_size, n_samples)\n",
    "    n_batches = max(n_samples // batch_size, 1)  # Ensure at least 1 batch\n",
    "    \n",
    "    print(\"Training Progress:\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Shuffle the data\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        # Mini-batch training\n",
    "        for batch in range(n_batches):\n",
    "            start_idx = batch * batch_size\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            \n",
    "            X_batch = X_shuffled[start_idx:end_idx]\n",
    "            y_batch = y_shuffled[start_idx:end_idx]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred, cache = forward_propagation(X_batch, parameters, layers)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = compute_loss(y_batch, y_pred)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grads = back_propagation(y_batch, parameters, cache, layers)\n",
    "            \n",
    "            # Update parameters\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 10 == 0:\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "\n",
    "# Cell 6: Prediction Function\n",
    "def predict(X, parameters, layers):\n",
    "    y_pred, _ = forward_propagation(X, parameters, layers)\n",
    "    return y_pred > 0.5\n",
    "\n",
    "# Cell 7: Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress:\n",
      "Epoch 0/1000, Loss: 0.0000\n",
      "Epoch 10/1000, Loss: 0.0000\n",
      "Epoch 20/1000, Loss: 0.0000\n",
      "Epoch 30/1000, Loss: 0.0000\n",
      "Epoch 40/1000, Loss: 0.0000\n",
      "Epoch 50/1000, Loss: 0.0000\n",
      "Epoch 60/1000, Loss: 0.0000\n",
      "Epoch 70/1000, Loss: 0.0000\n",
      "Epoch 80/1000, Loss: 0.0000\n",
      "Epoch 90/1000, Loss: 0.0000\n",
      "Epoch 100/1000, Loss: 0.0000\n",
      "Epoch 110/1000, Loss: 0.0000\n",
      "Epoch 120/1000, Loss: 0.0000\n",
      "Epoch 130/1000, Loss: 0.0000\n",
      "Epoch 140/1000, Loss: 0.0000\n",
      "Epoch 150/1000, Loss: 0.0000\n",
      "Epoch 160/1000, Loss: 0.0000\n",
      "Epoch 170/1000, Loss: 0.0000\n",
      "Epoch 180/1000, Loss: 0.0000\n",
      "Epoch 190/1000, Loss: 0.0000\n",
      "Epoch 200/1000, Loss: 0.0000\n",
      "Epoch 210/1000, Loss: 0.0000\n",
      "Epoch 220/1000, Loss: 0.0000\n",
      "Epoch 230/1000, Loss: 0.0000\n",
      "Epoch 240/1000, Loss: 0.0000\n",
      "Epoch 250/1000, Loss: 0.0000\n",
      "Epoch 260/1000, Loss: 0.0000\n",
      "Epoch 270/1000, Loss: 0.0000\n",
      "Epoch 280/1000, Loss: 0.0000\n",
      "Epoch 290/1000, Loss: 0.0000\n",
      "Epoch 300/1000, Loss: 0.0000\n",
      "Epoch 310/1000, Loss: 0.0000\n",
      "Epoch 320/1000, Loss: 0.0000\n",
      "Epoch 330/1000, Loss: 0.0000\n",
      "Epoch 340/1000, Loss: 0.0000\n",
      "Epoch 350/1000, Loss: 0.0000\n",
      "Epoch 360/1000, Loss: 0.0000\n",
      "Epoch 370/1000, Loss: 0.0000\n",
      "Epoch 380/1000, Loss: 0.0000\n",
      "Epoch 390/1000, Loss: 0.0000\n",
      "Epoch 400/1000, Loss: 0.0000\n",
      "Epoch 410/1000, Loss: 0.0000\n",
      "Epoch 420/1000, Loss: 0.0000\n",
      "Epoch 430/1000, Loss: 0.0000\n",
      "Epoch 440/1000, Loss: 0.0000\n",
      "Epoch 450/1000, Loss: 0.0000\n",
      "Epoch 460/1000, Loss: 0.0000\n",
      "Epoch 470/1000, Loss: 0.0000\n",
      "Epoch 480/1000, Loss: 0.0000\n",
      "Epoch 490/1000, Loss: 0.0000\n",
      "Epoch 500/1000, Loss: 0.0000\n",
      "Epoch 510/1000, Loss: 0.0000\n",
      "Epoch 520/1000, Loss: 0.0000\n",
      "Epoch 530/1000, Loss: 0.0000\n",
      "Epoch 540/1000, Loss: 0.0000\n",
      "Epoch 550/1000, Loss: 0.0000\n",
      "Epoch 560/1000, Loss: 0.0000\n",
      "Epoch 570/1000, Loss: 0.0000\n",
      "Epoch 580/1000, Loss: 0.0000\n",
      "Epoch 590/1000, Loss: 0.0000\n",
      "Epoch 600/1000, Loss: 0.0000\n",
      "Epoch 610/1000, Loss: 0.0000\n",
      "Epoch 620/1000, Loss: 0.0000\n",
      "Epoch 630/1000, Loss: 0.0000\n",
      "Epoch 640/1000, Loss: 0.0000\n",
      "Epoch 650/1000, Loss: 0.0000\n",
      "Epoch 660/1000, Loss: 0.0000\n",
      "Epoch 670/1000, Loss: 0.0000\n",
      "Epoch 680/1000, Loss: 0.0000\n",
      "Epoch 690/1000, Loss: 0.0000\n",
      "Epoch 700/1000, Loss: 0.0000\n",
      "Epoch 710/1000, Loss: 0.0000\n",
      "Epoch 720/1000, Loss: 0.0000\n",
      "Epoch 730/1000, Loss: 0.0000\n",
      "Epoch 740/1000, Loss: 0.0000\n",
      "Epoch 750/1000, Loss: 0.0000\n",
      "Epoch 760/1000, Loss: 0.0000\n",
      "Epoch 770/1000, Loss: 0.0000\n",
      "Epoch 780/1000, Loss: 0.0000\n",
      "Epoch 790/1000, Loss: 0.0000\n",
      "Epoch 800/1000, Loss: 0.0000\n",
      "Epoch 810/1000, Loss: 0.0000\n",
      "Epoch 820/1000, Loss: 0.0000\n",
      "Epoch 830/1000, Loss: 0.0000\n",
      "Epoch 840/1000, Loss: 0.0000\n",
      "Epoch 850/1000, Loss: 0.0000\n",
      "Epoch 860/1000, Loss: 0.0000\n",
      "Epoch 870/1000, Loss: 0.0000\n",
      "Epoch 880/1000, Loss: 0.0000\n",
      "Epoch 890/1000, Loss: 0.0000\n",
      "Epoch 900/1000, Loss: 0.0000\n",
      "Epoch 910/1000, Loss: 0.0000\n",
      "Epoch 920/1000, Loss: 0.0000\n",
      "Epoch 930/1000, Loss: 0.0000\n",
      "Epoch 940/1000, Loss: 0.0000\n",
      "Epoch 950/1000, Loss: 0.0000\n",
      "Epoch 960/1000, Loss: 0.0000\n",
      "Epoch 970/1000, Loss: 0.0000\n",
      "Epoch 980/1000, Loss: 0.0000\n",
      "Epoch 990/1000, Loss: 0.0000\n",
      "Predictions: [[ True]\n",
      " [ True]\n",
      " [ True]\n",
      " [ True]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define example data\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Shape: (4, 2)\n",
    "y = np.array([[0], [1], [1], [1]])  # Shape: (4, 1)\n",
    "\n",
    "# Define and train the network\n",
    "layers = [2, 3, 4, 4, 1]\n",
    "parameters = train_network(X, y, layers)\n",
    "\n",
    "# Make predictions\n",
    "predictions = predict(X, parameters, layers)\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/emnist-mnist-train.csv\")\n",
    "test = pd.read_csv(\"data/emnist-mnist-test.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59999, 785)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 785)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from CSV files...\n",
      "Training data shape: (59999, 784)\n",
      "Training labels shape: (59999, 10)\n",
      "Number of classes: 10\n",
      "\n",
      "Training the network...\n",
      "Training Progress:\n",
      "Epoch 0/100, Loss: 2.2988\n",
      "Epoch 10/100, Loss: 0.1662\n",
      "Epoch 20/100, Loss: 0.1008\n",
      "Epoch 30/100, Loss: 0.0691\n",
      "Epoch 40/100, Loss: 0.0497\n",
      "Epoch 50/100, Loss: 0.0368\n",
      "Epoch 60/100, Loss: 0.0279\n",
      "Epoch 70/100, Loss: 0.0216\n",
      "Epoch 80/100, Loss: 0.0168\n",
      "Epoch 90/100, Loss: 0.0132\n",
      "\n",
      "Test Accuracy: 97.91%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Now use the new function name in your training code:\n",
    "print(\"Loading dataset from CSV files...\")\n",
    "train_data = pd.read_csv(\"data/emnist-mnist-train.csv\")\n",
    "test_data = pd.read_csv(\"data/emnist-mnist-test.csv\")\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, 1:].values  # All columns except first\n",
    "y_train = train_data.iloc[:, 0].values   # First column contains labels\n",
    "\n",
    "X_test = test_data.iloc[:, 1:].values\n",
    "y_test = test_data.iloc[:, 0].values\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train_one_hot = to_one_hot(y_train, num_classes=num_classes)\n",
    "y_test_one_hot = to_one_hot(y_test, num_classes=num_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train_one_hot.shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Define network architecture\n",
    "input_size = X_train.shape[1]  # 784 features\n",
    "layers = [input_size, 256, 128, num_classes]\n",
    "\n",
    "# Train the network using the new function name\n",
    "print(\"\\nTraining the network...\")\n",
    "parameters = train_network(X_train, y_train_one_hot, \n",
    "                         layers=layers,\n",
    "                         learning_rate=0.01,\n",
    "                         epochs=100,\n",
    "                         batch_size=128)\n",
    "\n",
    "# Test the model\n",
    "predictions = predict(X_test, parameters, layers)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "actual_classes = y_test\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_classes == actual_classes)\n",
    "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
